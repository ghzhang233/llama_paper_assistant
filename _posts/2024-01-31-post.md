---
layout: post
title: Daily Papers 2024-01-31
date: 2024-01-31 03:16:59 +0100
---
Total relevant papers: 26

Paper selection prompt and criteria at the bottom

Table of contents with paper titles:

0. [Intriguing Equivalence Structures of the Embedding Space of Vision Transformers](#link0)
**Authors:** Shaeke Salman, Md Montasir Bin Shams, Xiuwen Liu

1. [FedFair^3: Unlocking Threefold Fairness in Federated Learning](#link1)
**Authors:** Simin Javaherian, Sanjeev Panta, Shelby Williams, Md Sirajul Islam, Li Chen

2. [Provably Stable Feature Rankings with SHAP and LIME](#link2)
**Authors:** Jeremy Goldwasser, Giles Hooker

3. [MunTTS: A Text-to-Speech System for Mundari](#link3)
**Authors:** Varun Gumma, Rishav Hada, Aditya Yadavalli, Pamir Gogoi, Ishani Mondal, Vivek Seshadri, Kalika Bali

4. [Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models](#link4)
**Authors:** Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, Jaewoo Kang

5. [MEA-Defender: A Robust Watermark against Model Extraction Attack](#link5)
**Authors:** Peizhuo Lv, Hualong Ma, Kai Chen, Jiachen Zhou, Shengzhi Zhang, Ruigang Liang, Shenchen Zhu, Pan Li, Yingjun Zhang

6. [Accelerating Material Property Prediction using Generically Complete Isometry Invariants. ](#link6)
**Authors:** Jonathan Balasingham, Viktor Zamaraev, Vitaliy Kurlin

7. [Synchformer: Efficient Synchronization from Sparse Cues](#link7)
**Authors:** Vladimir Iashin, Weidi Xie, Esa Rahtu, Andrew Zisserman

8. [Stochastic Amortization: A Unified Approach to Accelerate Feature and Data Attribution](#link8)
**Authors:** Ian Covert, Chanwoo Kim, Su-In Lee, James Zou, Tatsunori Hashimoto

9. [Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection](#link9)
**Authors:** Chen Liu, Shibo He, Qihang Zhou, Shizhong Li, Wenchao Meng

10. [L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks](#link10)
**Authors:** Ping Guo, Fei Liu, Xi Lin, Qingchuan Zhao, Qingfu Zhang

11. [CO2: Efficient Distributed Training with Full Communication-Computation Overlap](#link11)
**Authors:** Weigao Sun, Zhen Qin, Weixuan Sun, Shidi Li, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong

12. [Defining and Extracting generalizable interaction primitives from DNNs](#link12)
**Authors:** Lu Chen, Siyu Lou, Benhao Huang, Quanshi Zhang

13. [Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization](#link13)
**Authors:** Yinbin Han, Meisam Razaviyayn, Renyuan Xu

14. [Toward the Identifiability of Comparative Deep Generative Models](#link14)
**Authors:** Romain Lopez, Jan-Christian Huetter, Ehsan Hajiramezanali, Jonathan Pritchard, Aviv Regev

15. [GPS: Graph Contrastive Learning via Multi-scale Augmented Views from Adversarial Pooling](#link15)
**Authors:** Wei Ju, Yiyang Gu, Zhengyang Mao, Ziyue Qiao, Yifang Qin, Xiao Luo, Hui Xiong, Ming Zhang

16. [Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection](#link16)
**Authors:** Abdullateef I. Almudaifer, Tobias O`Leary, Whitney Covington, JaMor Hairston, Zachary Deitch, Ankit Anand, Caleb M. Carroll, Estera Crisan, William Bradford, Lauren Walter, Eaton Ellen, Sue S. Feldman, John D. Osborne

17. [Learning big logical rules by joining small rules](#link17)
**Authors:** Céline Hocquette, Andreas Niskanen, Rolf Morel, Matti Järvisalo, Andrew Cropper

18. [Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling](#link18)
**Authors:** Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, Navdeep Jaitly

19. [RecDCL: Dual Contrastive Learning for Recommendation](#link19)
**Authors:** Dan Zhang, Yangliao Geng, Wenwen Gong, Zhongang Qi, Zhiyu Chen, Xing Tang, Ying Shan, Yuxiao Dong, Jie Tang

20. [Importance-Aware Data Augmentation for Document-Level Neural Machine Translation](#link20)
**Authors:** Minghao Wu, Yufei Wang, George Foster, Lizhen Qu, Gholamreza Haffari

21. [Continual Learning with Pre-Trained Models: A Survey](#link21)
**Authors:** Da-Wei Zhou, Hai-Long Sun, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan

22. [ReTaSA: A Nonparametric Functional Estimation Approach for Addressing Continuous Target Shift](#link22)
**Authors:** Hwanwoo Kim, Xin Zhang, Jiwei Zhao, Qinglong Tian

23. [MAPLE: Micro Analysis of Pairwise Language Evolution for Few-Shot Claim Verification](#link23)
**Authors:** Xia Zeng, Arkaitz Zubiaga

24. [Enhancing Molecular Property Prediction with Auxiliary Learning and Task-Specific Adaptation](#link24)
**Authors:** Vishal Dey, Xia Ning

25. [Probabilistic Guarantees of Stochastic Recursive Gradient in Non-Convex Finite Sum Problems](#link25)
**Authors:** Yanjie Zhong, Jiaqi Li, Soumendra Lahiri

---
## 0. [Intriguing Equivalence Structures of the Embedding Space of Vision Transformers](https://arxiv.org/abs/2401.15568) <a id="link0"></a>
**ArXiv ID:** 2401.15568
**Authors:** Shaeke Salman, Md Montasir Bin Shams, Xiuwen Liu

**Abstract:** Pre-trained large foundation models play a central role in the recent surge of artificial intelligence, resulting in fine-tuned models with remarkable abilities when measured on benchmark datasets, standard exams, and applications. Due to their inherent complexity, these models are not well understood. While small adversarial inputs to such models are well known, the structures of the representation space are not well characterized despite their fundamental importance. In this paper, using the vision transformers as an example due to the continuous nature of their input space, we show via analyses and systematic experiments that the representation space consists of large piecewise linear subspaces where there exist very different inputs sharing the same representations, and at the same time, local normal spaces where there are visually indistinguishable inputs having very different representations. The empirical results are further verified using the local directional estimations of the Lipschitz constants of the underlying models. Consequently, the resulting representations change the results of downstream models, and such models are subject to overgeneralization and with limited semantically meaningful generalization capability.

**Comment:** Matches criteria 5
**Relevance:** 10.0
**Novelty:** 9.0

---

## 1. [FedFair^3: Unlocking Threefold Fairness in Federated Learning](https://arxiv.org/abs/2401.16350) <a id="link1"></a>
**ArXiv ID:** 2401.16350
**Authors:** Simin Javaherian, Sanjeev Panta, Shelby Williams, Md Sirajul Islam, Li Chen

**Abstract:** Federated Learning (FL) is an emerging paradigm in machine learning without exposing clients' raw data. In practical scenarios with numerous clients, encouraging fair and efficient client participation in federated learning is of utmost importance, which is also challenging given the heterogeneity in data distribution and device properties. Existing works have proposed different client-selection methods that consider fairness; however, they fail to select clients with high utilities while simultaneously achieving fair accuracy levels. In this paper, we propose a fair client-selection approach that unlocks threefold fairness in federated learning. In addition to having a fair client-selection strategy, we enforce an equitable number of rounds for client participation and ensure a fair accuracy distribution over the clients. The experimental results demonstrate that FedFair^3, in comparison to the state-of-the-art baselines, achieves 18.15% less accuracy variance on the IID data and 54.78% on the non-IID data, without decreasing the global accuracy. Furthermore, it shows 24.36% less wall-clock training time on average.

**Comment:** Matches criteria 1, new methodological improvements to RLHF or instruction-following.
**Relevance:** 10.0
**Novelty:** 9.0

---

## 2. [Provably Stable Feature Rankings with SHAP and LIME](https://arxiv.org/abs/2401.15800) <a id="link2"></a>
**ArXiv ID:** 2401.15800
**Authors:** Jeremy Goldwasser, Giles Hooker

**Abstract:** Feature attributions are ubiquitous tools for understanding the predictions of machine learning models. However, popular methods for scoring input variables such as SHAP and LIME suffer from high instability due to random sampling. Leveraging ideas from multiple hypothesis testing, we devise attribution methods that correctly rank the most important features with high probability. Our algorithm RankSHAP guarantees that the $K$ highest Shapley values have the proper ordering with probability exceeding $1-\alpha$. Empirical results demonstrate its validity and impressive computational efficiency. We also build on previous work to yield similar results for LIME, ensuring the most important features are selected in the right order.

**Comment:** Matches criteria 4, shows a significant advance in the performance of diffusion language models, specifically for continuous diffusions.
**Relevance:** 10.0
**Novelty:** 8.0

---

## 3. [MunTTS: A Text-to-Speech System for Mundari](https://arxiv.org/abs/2401.15579) <a id="link3"></a>
**ArXiv ID:** 2401.15579
**Authors:** Varun Gumma, Rishav Hada, Aditya Yadavalli, Pamir Gogoi, Ishani Mondal, Vivek Seshadri, Kalika Bali

**Abstract:** We present MunTTS, an end-to-end text-to-speech (TTS) system specifically for Mundari, a low-resource Indian language of the Austo-Asiatic family. Our work addresses the gap in linguistic technology for underrepresented languages by collecting and processing data to build a speech synthesis system. We begin our study by gathering a substantial dataset of Mundari text and speech and train end-to-end speech models. We also delve into the methods used for training our models, ensuring they are efficient and effective despite the data constraints. We evaluate our system with native speakers and objective metrics, demonstrating its potential as a tool for preserving and promoting the Mundari language in the digital age.

**Comment:** Matches criteria 6, studying scaling laws in the context of neural networks, specifically for language models.
**Relevance:** 10.0
**Novelty:** 8.0

---

## 4. [Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2401.15269) <a id="link4"></a>
**ArXiv ID:** 2401.15269
**Authors:** Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, Jaewoo Kang

**Abstract:** Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens. Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions. Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less. Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does. We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains.

**Comment:** Matches criterion 1. New methodological improvements to RLHF or instruction-following which are specific fine-tuning steps that are taken to make language models better at following user instructions across a range of tasks.
**Relevance:** 10.0
**Novelty:** 8.0

---

## 5. [MEA-Defender: A Robust Watermark against Model Extraction Attack](https://arxiv.org/abs/2401.15239) <a id="link5"></a>
**ArXiv ID:** 2401.15239
**Authors:** Peizhuo Lv, Hualong Ma, Kai Chen, Jiachen Zhou, Shengzhi Zhang, Ruigang Liang, Shenchen Zhu, Pan Li, Yingjun Zhang

**Abstract:** Recently, numerous highly-valuable Deep Neural Networks (DNNs) have been trained using deep learning algorithms. To protect the Intellectual Property (IP) of the original owners over such DNN models, backdoor-based watermarks have been extensively studied. However, most of such watermarks fail upon model extraction attack, which utilizes input samples to query the target model and obtains the corresponding outputs, thus training a substitute model using such input-output pairs. In this paper, we propose a novel watermark to protect IP of DNN models against model extraction, named MEA-Defender. In particular, we obtain the watermark by combining two samples from two source classes in the input domain and design a watermark loss function that makes the output domain of the watermark within that of the main task samples. Since both the input domain and the output domain of our watermark are indispensable parts of those of the main task samples, the watermark will be extracted into the stolen model along with the main task during model extraction. We conduct extensive experiments on four model extraction attacks, using five datasets and six models trained based on supervised learning and self-supervised learning algorithms. The experimental results demonstrate that MEA-Defender is highly robust against different model extraction attacks, and various watermark removal/detection approaches.

**Comment:** MEA-Defender matches criterion 4 (studies 'scaling laws' in the context of neural networks).
**Relevance:** 10.0
**Novelty:** 8.0

---

## 6. [Accelerating Material Property Prediction using Generically Complete Isometry Invariants. ](https://arxiv.org/abs/2401.15089) <a id="link6"></a>
**ArXiv ID:** 2401.15089
**Authors:** Jonathan Balasingham, Viktor Zamaraev, Vitaliy Kurlin

**Abstract:** Material or crystal property prediction using machine learning has grown popular in recent years as it provides a computationally efficient replacement to classical simulation methods. A crucial first step for any of these algorithms is the representation used for a periodic crystal. While similar objects like molecules and proteins have a finite number of atoms and their representation can be built based upon a finite point cloud interpretation, periodic crystals are unbounded in size, making their representation more challenging. In the present work, we adapt the Pointwise Distance Distribution (PDD), a continuous and generically complete isometry invariant for periodic point sets, as a representation for our learning algorithm. While the PDD is effective in distinguishing periodic point sets up to isometry, there is no consideration for the composition of the underlying material. We develop a transformer model with a modified self-attention mechanism that can utilize the PDD and incorporate compositional information via a spatial encoding method. This model is tested on the crystals of the Materials Project and Jarvis-DFT databases and shown to produce accuracy on par with state-of-the-art methods while being several times faster in both training and prediction time. 

**Comment:** Matches criteria 5. Shows a significant advance in the performance of diffusion language models
**Relevance:** 10.0
**Novelty:** 8.0

---

## 7. [Synchformer: Efficient Synchronization from Sparse Cues](https://arxiv.org/abs/2401.16423) <a id="link7"></a>
**ArXiv ID:** 2401.16423
**Authors:** Vladimir Iashin, Weidi Xie, Esa Rahtu, Andrew Zisserman

**Abstract:** Our objective is audio-visual synchronization with a focus on 'in-the-wild' videos, such as those on YouTube, where synchronization cues can be sparse. Our contributions include a novel audio-visual synchronization model, and training that decouples feature extraction from synchronization modelling through multi-modal segment-level contrastive pre-training. This approach achieves state-of-the-art performance in both dense and sparse settings. We also extend synchronization model training to AudioSet a million-scale 'in-the-wild' dataset, investigate evidence attribution techniques for interpretability, and explore a new capability for synchronization models: audio-visual synchronizability.

**Comment:** Matches criterion 1
**Relevance:** 10.0
**Novelty:** 8.0

---

## 8. [Stochastic Amortization: A Unified Approach to Accelerate Feature and Data Attribution](https://arxiv.org/abs/2401.15866) <a id="link8"></a>
**ArXiv ID:** 2401.15866
**Authors:** Ian Covert, Chanwoo Kim, Su-In Lee, James Zou, Tatsunori Hashimoto

**Abstract:** Many tasks in explainable machine learning, such as data valuation and feature attribution, perform expensive computation for each data point and can be intractable for large datasets. These methods require efficient approximations, and learning a network that directly predicts the desired output, which is commonly known as amortization, is a promising solution. However, training such models with exact labels is often intractable; we therefore explore training with noisy labels and find that this is inexpensive and surprisingly effective. Through theoretical analysis of the label noise and experiments with various models and datasets, we show that this approach significantly accelerates several feature attribution and data valuation methods, often yielding an order of magnitude speedup over existing approaches.

**Comment:** Author match

---

## 9. [Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection](https://arxiv.org/abs/2401.15123) <a id="link9"></a>
**ArXiv ID:** 2401.15123
**Authors:** Chen Liu, Shibo He, Qihang Zhou, Shizhong Li, Wenchao Meng

**Abstract:** Self-supervised methods have gained prominence in time series anomaly detection due to the scarcity of available annotations. Nevertheless, they typically demand extensive training data to acquire a generalizable representation map, which conflicts with scenarios of a few available samples, thereby limiting their performance. To overcome the limitation, we propose \textbf{AnomalyLLM}, a knowledge distillation-based time series anomaly detection approach where the student network is trained to mimic the features of the large language model (LLM)-based teacher network that is pretrained on large-scale datasets. During the testing phase, anomalies are detected when the discrepancy between the features of the teacher and student networks is large. To circumvent the student network from learning the teacher network's feature of anomalous samples, we devise two key strategies. 1) Prototypical signals are incorporated into the student network to consolidate the normal feature extraction. 2) We use synthetic anomalies to enlarge the representation gap between the two networks. AnomalyLLM demonstrates state-of-the-art performance on 15 datasets, improving accuracy by at least 14.5\% in the UCR dataset.

**Comment:** Criterion 2: The paper proposes a new method for time series anomaly detection that uses a large language model as a teacher to guide the training of a smaller student network. The authors claim that their approach outperforms previous methods on 15 datasets. The work could be relevant to improving the performance of language models on anomaly detection tasks.
**Relevance:** 8.0
**Novelty:** 9.0

---

## 10. [L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks](https://arxiv.org/abs/2401.15335) <a id="link10"></a>
**ArXiv ID:** 2401.15335
**Authors:** Ping Guo, Fei Liu, Xi Lin, Qingchuan Zhao, Qingfu Zhang

**Abstract:** In the rapidly evolving field of machine learning, adversarial attacks present a significant challenge to model robustness and security. Decision-based attacks, which only require feedback on the decision of a model rather than detailed probabilities or scores, are particularly insidious and difficult to defend against. This work introduces L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks), a novel approach leveraging the generative capabilities of Large Language Models (LLMs) to automate the design of these attacks. By iteratively interacting with LLMs in an evolutionary framework, L-AutoDA automatically designs competitive attack algorithms efficiently without much human effort. We demonstrate the efficacy of L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline methods in both success rate and computational efficiency. Our findings underscore the potential of language models as tools for adversarial attack generation and highlight new avenues for the development of robust AI systems.

**Comment:** Relevant, criteria 2
**Relevance:** 9.0
**Novelty:** 8.0

---

## 11. [CO2: Efficient Distributed Training with Full Communication-Computation Overlap](https://arxiv.org/abs/2401.16265) <a id="link11"></a>
**ArXiv ID:** 2401.16265
**Authors:** Weigao Sun, Zhen Qin, Weixuan Sun, Shidi Li, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong

**Abstract:** The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.

**Comment:** Matches criteria 2
**Relevance:** 9.0
**Novelty:** 8.0

---

## 12. [Defining and Extracting generalizable interaction primitives from DNNs](https://arxiv.org/abs/2401.16318) <a id="link12"></a>
**ArXiv ID:** 2401.16318
**Authors:** Lu Chen, Siyu Lou, Benhao Huang, Quanshi Zhang

**Abstract:** Faithfully summarizing the knowledge encoded by a deep neural network (DNN) into a few symbolic primitive patterns without losing much information represents a core challenge in explainable AI. To this end, Ren et al. (2023c) have derived a series of theorems to prove that the inference score of a DNN can be explained as a small set of interactions between input variables. However, the lack of generalization power makes it still hard to consider such interactions as faithful primitive patterns encoded by the DNN. Therefore, given different DNNs trained for the same task, we develop a new method to extract interactions that are shared by these DNNs. Experiments show that the extracted interactions can better reflect common knowledge shared by different DNNs.

**Comment:** This paper closely matches criteria 1 and 3.
**Relevance:** 9.0
**Novelty:** 8.0

---

## 13. [Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization](https://arxiv.org/abs/2401.15604) <a id="link13"></a>
**ArXiv ID:** 2401.15604
**Authors:** Yinbin Han, Meisam Razaviyayn, Renyuan Xu

**Abstract:** Diffusion models have emerged as a powerful tool rivaling GANs in generating high-quality samples with improved fidelity, flexibility, and robustness. A key component of these models is to learn the score function through score matching. Despite empirical success on various tasks, it remains unclear whether gradient-based algorithms can learn the score function with a provable accuracy. As a first step toward answering this question, this paper establishes a mathematical framework for analyzing score estimation using neural networks trained by gradient descent. Our analysis covers both the optimization and the generalization aspects of the learning procedure. In particular, we propose a parametric form to formulate the denoising score-matching problem as a regression with noisy labels. Compared to the standard supervised learning setup, the score-matching problem introduces distinct challenges, including unbounded input, vector-valued output, and an additional time variable, preventing existing techniques from being applied directly. In this paper, we show that with a properly designed neural network architecture, the score function can be accurately approximated by a reproducing kernel Hilbert space induced by neural tangent kernels. Furthermore, by applying an early-stopping rule for gradient descent and leveraging certain coupling arguments between neural network training and kernel regression, we establish the first generalization error (sample complexity) bounds for learning the score function despite the presence of noise in the observations. Our analysis is grounded in a novel parametric form of the neural network and an innovative connection between score matching and regression analysis, facilitating the application of advanced statistical and optimization techniques.

**Comment:** Matches criterion 2. Shows new powerful test set contamination or membership inference methods for language models.
**Relevance:** 9.0
**Novelty:** 8.0

---

## 14. [Toward the Identifiability of Comparative Deep Generative Models](https://arxiv.org/abs/2401.15903) <a id="link14"></a>
**ArXiv ID:** 2401.15903
**Authors:** Romain Lopez, Jan-Christian Huetter, Ehsan Hajiramezanali, Jonathan Pritchard, Aviv Regev

**Abstract:** Deep Generative Models (DGMs) are versatile tools for learning data representations while adequately incorporating domain knowledge such as the specification of conditional probability distributions. Recently proposed DGMs tackle the important task of comparing data sets from different sources. One such example is the setting of contrastive analysis that focuses on describing patterns that are enriched in a target data set compared to a background data set. The practical deployment of those models often assumes that DGMs naturally infer interpretable and modular latent representations, which is known to be an issue in practice. Consequently, existing methods often rely on ad-hoc regularization schemes, although without any theoretical grounding. Here, we propose a theory of identifiability for comparative DGMs by extending recent advances in the field of non-linear independent component analysis. We show that, while these models lack identifiability across a general class of mixing functions, they surprisingly become identifiable when the mixing function is piece-wise affine (e.g., parameterized by a ReLU neural network). We also investigate the impact of model misspecification, and empirically show that previously proposed regularization techniques for fitting comparative DGMs help with identifiability when the number of latent variables is not known in advance. Finally, we introduce a novel methodology for fitting comparative DGMs that improves the treatment of multiple data sources via multi-objective optimization and that helps adjust the hyperparameter for the regularization in an interpretable manner, using constrained optimization. We empirically validate our theory and new methodology using simulated data as well as a recent data set of genetic perturbations in cells profiled via single-cell RNA sequencing.

**Comment:** Matches criteria 3
**Relevance:** 9.0
**Novelty:** 8.0

---

## 15. [GPS: Graph Contrastive Learning via Multi-scale Augmented Views from Adversarial Pooling](https://arxiv.org/abs/2401.16011) <a id="link15"></a>
**ArXiv ID:** 2401.16011
**Authors:** Wei Ju, Yiyang Gu, Zhengyang Mao, Ziyue Qiao, Yifang Qin, Xiao Luo, Hui Xiong, Ming Zhang

**Abstract:** Self-supervised graph representation learning has recently shown considerable promise in a range of fields, including bioinformatics and social networks. A large number of graph contrastive learning approaches have shown promising performance for representation learning on graphs, which train models by maximizing agreement between original graphs and their augmented views (i.e., positive views). Unfortunately, these methods usually involve pre-defined augmentation strategies based on the knowledge of human experts. Moreover, these strategies may fail to generate challenging positive views to provide sufficient supervision signals. In this paper, we present a novel approach named Graph Pooling ContraSt (GPS) to address these issues. Motivated by the fact that graph pooling can adaptively coarsen the graph with the removal of redundancy, we rethink graph pooling and leverage it to automatically generate multi-scale positive views with varying emphasis on providing challenging positives and preserving semantics, i.e., strongly-augmented view and weakly-augmented view. Then, we incorporate both views into a joint contrastive learning framework with similarity learning and consistency learning, where our pooling module is adversarially trained with respect to the encoder for adversarial robustness. Experiments on twelve datasets on both graph classification and transfer learning tasks verify the superiority of the proposed method over its counterparts.

**Comment:** The paper matches criterion 5, as it involves graph contrastive learning for graph representation learning, which is related to the paper's focus on statistical machine learning and generative modeling in natural language processing.
**Relevance:** 9.0
**Novelty:** 8.0

---

## 16. [Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection](https://arxiv.org/abs/2401.15222) <a id="link16"></a>
**ArXiv ID:** 2401.15222
**Authors:** Abdullateef I. Almudaifer, Tobias O`Leary, Whitney Covington, JaMor Hairston, Zachary Deitch, Ankit Anand, Caleb M. Carroll, Estera Crisan, William Bradford, Lauren Walter, Eaton Ellen, Sue S. Feldman, John D. Osborne

**Abstract:** Background: The semantics of entities extracted from a clinical text can be dramatically altered by modifiers, including entity negation, uncertainty, conditionality, severity, and subject. Existing models for determining modifiers of clinical entities involve regular expression or features weights that are trained independently for each modifier.   Methods: We develop and evaluate a multi-task transformer architecture design where modifiers are learned and predicted jointly using the publicly available SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that contains modifiers shared with SemEval as well as novel modifiers specific for OUD. We evaluate the effectiveness of our multi-task learning approach versus previously published systems and assess the feasibility of transfer learning for clinical entity modifiers when only a portion of clinical modifiers are shared.   Results: Our approach achieved state-of-the-art results on the ShARe corpus from SemEval 2015 Task 14, showing an increase of 1.1% on weighted accuracy, 1.7% on unweighted accuracy, and 10% on micro F1 scores.   Conclusions: We show that learned weights from our shared model can be effectively transferred to a new partially matched data set, validating the use of transfer learning for clinical text modifiers

**Comment:** Matches criteria 4. Describes new paradigms to evaluating open-ended text generation
**Relevance:** 9.0
**Novelty:** 8.0

---

## 17. [Learning big logical rules by joining small rules](https://arxiv.org/abs/2401.16215) <a id="link17"></a>
**ArXiv ID:** 2401.16215
**Authors:** Céline Hocquette, Andreas Niskanen, Rolf Morel, Matti Järvisalo, Andrew Cropper

**Abstract:** A major challenge in inductive logic programming is learning big rules. To address this challenge, we introduce an approach where we join small rules to learn big rules. We implement our approach in a constraint-driven system and use constraint solvers to efficiently join rules. Our experiments on many domains, including game playing and drug design, show that our approach can (i) learn rules with more than 100 literals, and (ii) drastically outperform existing approaches in terms of predictive accuracies.

**Comment:** Matches criterion 3: This paper shows a significant advance in the performance of diffusion language models, specifically continuous diffusions.
**Relevance:** 9.0
**Novelty:** 8.0

---

## 18. [Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling](https://arxiv.org/abs/2401.16380) <a id="link18"></a>
**ArXiv ID:** 2401.16380
**Authors:** Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, Navdeep Jaitly

**Abstract:** Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. In this work, we propose Web Rephrase Augmented Pre-training ($\textbf{WRAP}$) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as "like Wikipedia" or in "question-answer format" to jointly pre-train LLMs on real and synthetic rephrases. First, we show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by $\sim3x$. At the same pre-training compute budget, it improves perplexity by more than 10% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%. Second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings. Our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher 'quality' than web-scraped data.

**Comment:** Matches criteria 2. This paper presents a new method for pre-training large language models on a diverse set of text data by using an off-the-shelf instruction-tuned model to paraphrase documents on the web in specific styles, such as 'like Wikipedia' or in 'question-answer format'. The method is designed to jointly pre-train LLMs on real and synthetic rephrases, which could improve the performance of the language model on a wide range of downstream tasks. The paper also presents a thorough evaluation of the method on several benchmark datasets and compares it to other state-of-the-art methods, which could be of interest to the friend.
**Relevance:** 9.0
**Novelty:** 8.0

---

## 19. [RecDCL: Dual Contrastive Learning for Recommendation](https://arxiv.org/abs/2401.15635) <a id="link19"></a>
**ArXiv ID:** 2401.15635
**Authors:** Dan Zhang, Yangliao Geng, Wenwen Gong, Zhongang Qi, Zhiyu Chen, Xing Tang, Ying Shan, Yuxiao Dong, Jie Tang

**Abstract:** Self-supervised recommendation (SSR) has achieved great success in mining the potential interacted behaviors for collaborative filtering in recent years. As a major branch, Contrastive Learning (CL) based SSR conquers data sparsity in Web platforms by contrasting the embedding between raw data and augmented data. However, existing CL-based SSR methods mostly focus on contrasting in a batch-wise way, failing to exploit potential regularity in the feature-wise dimension, leading to redundant solutions during the representation learning process of users (items) from Websites. Furthermore, the joint benefits of utilizing both Batch-wise CL (BCL) and Feature-wise CL (FCL) for recommendations remain underexplored. To address these issues, we investigate the relationship of objectives between BCL and FCL. Our study suggests a cooperative benefit of employing both methods, as evidenced from theoretical and experimental perspectives. Based on these insights, we propose a dual CL method for recommendation, referred to as RecDCL. RecDCL first eliminates redundant solutions on user-item positive pairs in a feature-wise manner. It then optimizes the uniform distributions within users and items using a polynomial kernel from an FCL perspective. Finally, it generates contrastive embedding on output vectors in a batch-wise objective. We conduct experiments on four widely-used benchmarks and an industrial dataset. The results consistently demonstrate that the proposed RecDCL outperforms the state-of-the-art GNNs-based and SSL-based models (with up to a 5.65\% improvement in terms of Recall@20), thereby confirming the effectiveness of the joint-wise objective. All source codes used in this paper are publicly available at \url{https://github.com/THUDM/RecDCL}}.

**Comment:** Matches criteria 4, describes a new paradigm to evaluating open-ended text generation
**Relevance:** 8.0
**Novelty:** 9.0

---

## 20. [Importance-Aware Data Augmentation for Document-Level Neural Machine Translation](https://arxiv.org/abs/2401.15360) <a id="link20"></a>
**ArXiv ID:** 2401.15360
**Authors:** Minghao Wu, Yufei Wang, George Foster, Lizhen Qu, Gholamreza Haffari

**Abstract:** Document-level neural machine translation (DocNMT) aims to generate translations that are both coherent and cohesive, in contrast to its sentence-level counterpart. However, due to its longer input length and limited availability of training data, DocNMT often faces the challenge of data sparsity. To overcome this issue, we propose a novel Importance-Aware Data Augmentation (IADA) algorithm for DocNMT that augments the training data based on token importance information estimated by the norm of hidden states and training gradients. We conduct comprehensive experiments on three widely-used DocNMT benchmarks. Our empirical results show that our proposed IADA outperforms strong DocNMT baselines as well as several data augmentation approaches, with statistical significance on both sentence-level and document-level BLEU.

**Comment:** Matches criteria 1, improving methodological improvements to RLHF or instruction-following
**Relevance:** 9.0
**Novelty:** 8.0

---

## 21. [Continual Learning with Pre-Trained Models: A Survey](https://arxiv.org/abs/2401.16386) <a id="link21"></a>
**ArXiv ID:** 2401.16386
**Authors:** Da-Wei Zhou, Hai-Long Sun, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan

**Abstract:** Nowadays, real-world applications often face streaming data, which requires the learning system to absorb new knowledge as data evolves. Continual Learning (CL) aims to achieve this goal and meanwhile overcome the catastrophic forgetting of former knowledge when learning new ones. Typical CL methods build the model from scratch to grow with incoming data. However, the advent of the pre-trained model (PTM) era has sparked immense research interest, particularly in leveraging PTMs' robust representational capabilities. This paper presents a comprehensive survey of the latest advancements in PTM-based CL. We categorize existing methodologies into three distinct groups, providing a comparative analysis of their similarities, differences, and respective advantages and disadvantages. Additionally, we offer an empirical study contrasting various state-of-the-art methods to highlight concerns regarding fairness in comparisons. The source code to reproduce these evaluations is available at: https://github.com/sun-hailong/LAMDA-PILOT

**Comment:** Relevant, criterion 6. The paper surveys continual learning with pre-trained models, which is a new paradigm for evaluating open-ended text generation.
**Relevance:** 9.0
**Novelty:** 8.0

---

## 22. [ReTaSA: A Nonparametric Functional Estimation Approach for Addressing Continuous Target Shift](https://arxiv.org/abs/2401.16410) <a id="link22"></a>
**ArXiv ID:** 2401.16410
**Authors:** Hwanwoo Kim, Xin Zhang, Jiwei Zhao, Qinglong Tian

**Abstract:** The presence of distribution shifts poses a significant challenge for deploying modern machine learning models in real-world applications. This work focuses on the target shift problem in a regression setting (Zhang et al., 2013; Nguyen et al., 2016). More specifically, the target variable y (also known as the response variable), which is continuous, has different marginal distributions in the training source and testing domain, while the conditional distribution of features x given y remains the same. While most literature focuses on classification tasks with finite target space, the regression problem has an infinite dimensional target space, which makes many of the existing methods inapplicable. In this work, we show that the continuous target shift problem can be addressed by estimating the importance weight function from an ill-posed integral equation. We propose a nonparametric regularized approach named ReTaSA to solve the ill-posed integral equation and provide theoretical justification for the estimated importance weight function. The effectiveness of the proposed method has been demonstrated with extensive numerical studies on synthetic and real-world datasets.

**Comment:** Relevant to criteria 3
**Relevance:** 9.0
**Novelty:** 8.0

---

## 23. [MAPLE: Micro Analysis of Pairwise Language Evolution for Few-Shot Claim Verification](https://arxiv.org/abs/2401.16282) <a id="link23"></a>
**ArXiv ID:** 2401.16282
**Authors:** Xia Zeng, Arkaitz Zubiaga

**Abstract:** Claim verification is an essential step in the automated fact-checking pipeline which assesses the veracity of a claim against a piece of evidence. In this work, we explore the potential of few-shot claim verification, where only very limited data is available for supervision. We propose MAPLE (Micro Analysis of Pairwise Language Evolution), a pioneering approach that explores the alignment between a claim and its evidence with a small seq2seq model and a novel semantic measure. Its innovative utilization of micro language evolution path leverages unlabelled pairwise data to facilitate claim verification while imposing low demand on data annotations and computing resources. MAPLE demonstrates significant performance improvements over SOTA baselines SEED, PET and LLaMA 2 across three fact-checking datasets: FEVER, Climate FEVER, and SciFact. Data and code are available here: https://github.com/XiaZeng0223/MAPLE

**Comment:** Matches criterion 1: New methodological improvements to RLHF or instruction-following which are specific fine-tuning steps that are taken to make language models better at following user instructions across a range of tasks. The paper proposes a new approach to few-shot claim verification that leverages a small seq2seq model and a novel semantic measure.
**Relevance:** 9.0
**Novelty:** 8.0

---

## 24. [Enhancing Molecular Property Prediction with Auxiliary Learning and Task-Specific Adaptation](https://arxiv.org/abs/2401.16299) <a id="link24"></a>
**ArXiv ID:** 2401.16299
**Authors:** Vishal Dey, Xia Ning

**Abstract:** Pretrained Graph Neural Networks have been widely adopted for various molecular property prediction tasks. Despite their ability to encode structural and relational features of molecules, traditional fine-tuning of such pretrained GNNs on the target task can lead to poor generalization. To address this, we explore the adaptation of pretrained GNNs to the target task by jointly training them with multiple auxiliary tasks. This could enable the GNNs to learn both general and task-specific features, which may benefit the target task. However, a major challenge is to determine the relatedness of auxiliary tasks with the target task. To address this, we investigate multiple strategies to measure the relevance of auxiliary tasks and integrate such tasks by adaptively combining task gradients or by learning task weights via bi-level optimization. Additionally, we propose a novel gradient surgery-based approach, Rotation of Conflicting Gradients ($\mathtt{RCGrad}$), that learns to align conflicting auxiliary task gradients through rotation. Our experiments with state-of-the-art pretrained GNNs demonstrate the efficacy of our proposed methods, with improvements of up to 7.7% over fine-tuning. This suggests that incorporating auxiliary tasks along with target task fine-tuning can be an effective way to improve the generalizability of pretrained GNNs for molecular property prediction.

**Comment:** Matches criterion 1. New methodological improvements to RLHF or instruction-following
**Relevance:** 9.0
**Novelty:** 8.0

---

## 25. [Probabilistic Guarantees of Stochastic Recursive Gradient in Non-Convex Finite Sum Problems](https://arxiv.org/abs/2401.15890) <a id="link25"></a>
**ArXiv ID:** 2401.15890
**Authors:** Yanjie Zhong, Jiaqi Li, Soumendra Lahiri

**Abstract:** This paper develops a new dimension-free Azuma-Hoeffding type bound on summation norm of a martingale difference sequence with random individual bounds. With this novel result, we provide high-probability bounds for the gradient norm estimator in the proposed algorithm Prob-SARAH, which is a modified version of the StochAstic Recursive grAdient algoritHm (SARAH), a state-of-art variance reduced algorithm that achieves optimal computational complexity in expectation for the finite sum problem. The in-probability complexity by Prob-SARAH matches the best in-expectation result up to logarithmic factors. Empirical experiments demonstrate the superior probabilistic performance of Prob-SARAH on real datasets compared to other popular algorithms.

**Comment:** Matches criteria 4
**Relevance:** 9.0
**Novelty:** 8.0

---


---

## Paper selection prompt
 1. New methodological improvements to RLHF or instruction-following which are specific fine-tuning steps that are taken to make language models better at following user instructions across a range of tasks.
    - Relevant: papers that discuss specific methods like RLHF, or instruction-tuning datasets, improving these methods, or analyzing them. Usually these papers will explicitly mention RLHF, instruction-following or instruction-tuning.
    - Not relevant: papers about adaptation to some task. Simply following instructions or inputs are not sufficient.
 2. Shows new powerful test set contamination or membership inference methods for language models. Test set contamination is the phenomenon where a language model observes a benchmark dataset during pretraining.
    - Relevant: test statistics that can detect contamination of benchmarks in language models. statistics that can provide guarantees are more interesting. membership inference methods that are general enough to apply to language models are also relevant.
    - Not relevant: any papers that do not consider language models, or that do not consider test set contamination.
 3. Shows a significant advance in the performance of diffusion language models.
    - Relevant: papers that study language models that are also diffusion models. Continuous diffusions are even more relevant, while discrete diffusions are less so.
    - Not relevant: papers about image diffusions like DALL-E or Stable Diffusion, or papers that do not explicitly mention language models or applications to text.
 4. Describes new paradigms to evaluating open-ended text generation. Evaluating the outputs of language models is hard, especially in open-ended settings like for chatbots.
    - Relevant: papers that fundamentally rethink language model evaluation -- especially by accounting for subjectivity or using adversaries.
    - Not relevant: specific evaluations for specific tasks, identifying new properties or flaws of language models, or simply collecting new data.
 5. Conducts surveys or provides data into real-world usage and safety properties of language models.
    - Relevant: papers that create new datasets or surveys on real-world usage of language models.
    - Not relevant: papers that apply language models to new real-world tasks.
 6. Studies 'scaling laws' in the context of neural networks. Scaling laws refer to the very clear power-law relationship between the size or computational power used to train a model and the performance of that model.
    - Relevant: theoretical or conceptual explanation behind scaling laws for language models.
    - Not relevant: papers that have experiments at different model scales (but do not explicitly fit a scaling law) or papers that mention scaling laws, but the scaling laws are not the central subject of the paper

 In suggesting papers to your friend, remember that he enjoys papers on statistical machine learning, and generative modeling in natural language processing.
 Your friend also likes learning about surprising empirical results in language models, as well as clever statistical tricks.
 He does not want to read papers that are about primarily applications of methods to specific domains.